{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing the.csv files\n",
    "pasta = r'D:\\OneDrive\\Documentos\\LabMol\\IC-Citotoxicidade\\DeepCytosafe multiparam\\Data\\teste'\n",
    "\n",
    "# Load all.csv files in the specified folder\n",
    "datasets = []\n",
    "for arquivo in os.listdir(pasta):\n",
    "    if arquivo.endswith('.csv'):\n",
    "        caminho_completo = os.path.join(pasta, arquivo)\n",
    "        df = pd.read_csv(caminho_completo)\n",
    "        datasets.append(df)\n",
    "\n",
    "# Function to tokenize texts using BERT\n",
    "def tokenize_with_bert(tokenizer, texts, max_length=128):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Prepare the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the datasets\n",
    "tokenized_datasets = []\n",
    "for df in datasets:\n",
    "    tokenized = tokenize_with_bert(tokenizer, df['final_smiles'].tolist())\n",
    "    labels = torch.tensor(df['Outcome'].values)\n",
    "    tokenized_datasets.append((tokenized['input_ids'], tokenized['attention_mask'], labels))\n",
    "\n",
    "# Concatenate the tokenized datasets\n",
    "input_ids = torch.cat([x[0] for x in tokenized_datasets])\n",
    "attention_masks = torch.cat([x[1] for x in tokenized_datasets])\n",
    "labels = torch.cat([x[2] for x in tokenized_datasets])\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.torch_save('train_loader.pt')\n",
    "val_loader.torch_save('val_loader.pt')\n",
    "test_loader.torch_save('test_loader.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskBERT(nn.Module):\n",
    "    def __init__(self, bert_model, num_labels_per_task):\n",
    "        super(MultiTaskBERT, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.num_labels_per_task = num_labels_per_task\n",
    "        self.classifier1 = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.num_labels_per_task[0])\n",
    "        )\n",
    "        self.classifier2 = nn.Sequential(\n",
    "            nn.Linear(self.bert.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.num_labels_per_task[1])\n",
    "        )\n",
    "        # Add more classifiers for additional tasks as needed\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, task_idx):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        if task_idx == 0:\n",
    "            logits = self.classifier1(pooled_output)\n",
    "        elif task_idx == 1:\n",
    "            logits = self.classifier2(pooled_output)\n",
    "        # Add more logits for additional tasks as needed\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do modelo e otimizador\n",
    "model = MultiTaskBERT(BertModel.from_pretrained('bert-base-uncased'), num_labels_per_task=[2, 2])\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use BCEWithLogitsLoss for binary classification\n",
    "\n",
    "# Função de treinamento\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask, task_idx=0)  # Assuming task_idx=0 for the first task\n",
    "        loss = criterion(logits, labels.float())  # Use float() to convert labels to float\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        predicted = torch.sigmoid(logits)  # Apply sigmoid to get probabilities\n",
    "        predicted = (predicted > 0.5).int()  # Convert probabilities to binary labels\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    \n",
    "    return total_loss / total_samples, total_correct / total_samples\n",
    "\n",
    "# Função de avaliação\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, task_idx=0)  # Assuming task_idx=0 for the first task\n",
    "            loss = criterion(logits, labels.float())  # Use float() to convert labels to float\n",
    "            \n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            predicted = torch.sigmoid(logits)  # Apply sigmoid to get probabilities\n",
    "            predicted = (predicted > 0.5).int()  # Convert probabilities to binary labels\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    return total_loss / total_samples, total_correct / total_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treinamento\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "          \n",
    "model.to(device)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = 0.0, 0.0\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for batch in tepoch:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask, task_idx=0)  # Assuming task_idx=0 for the first task\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            train_acc += correct\n",
    "            train_loss += (1 - (correct / labels.size(0))) * labels.size(0)\n",
    "            \n",
    "            # Define the loss as a PyTorch tensor\n",
    "            loss_tensor = torch.tensor(1 - (correct / labels.size(0)), requires_grad=True)\n",
    "            loss_tensor.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            tepoch.set_postfix({'loss': f'{loss_tensor.item():.4f}', 'acc': f'{correct / labels.size(0):.4f}'})\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc /= len(train_loader.dataset)\n",
    "    \n",
    "    val_acc = evaluate_model(model, val_loader, device)\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')  # Salva o melhor modelo\n",
    "\n",
    "# Avaliação final no conjunto de teste\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_acc = evaluate_model(model, test_loader, device)\n",
    "print(f'Final Test Acc: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
