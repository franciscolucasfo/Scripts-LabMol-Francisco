{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.SaltRemover import SaltRemover\n",
    "from rdkit.Chem import inchi as rd_inchi\n",
    "from chembl_structure_pipeline import standardizer\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "import os\n",
    "\n",
    "# Constantes\n",
    "VARIANCE_THRESHOLD = 0.2  # Threshold para variação de log em problemas de regressão\n",
    "\n",
    "# Instanciação do SaltRemover uma única vez\n",
    "remover = SaltRemover()\n",
    "\n",
    "def remove_salts(smile):\n",
    "    \"\"\"Processa a molécula, removendo sais e verificando validade.\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smile)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        # Utiliza a instância 'remover' para remover sais\n",
    "        cleaned_mol = remover.StripMol(mol, dontRemoveEverything=True)\n",
    "        if cleaned_mol.GetNumAtoms() > 2:\n",
    "            return Chem.MolToSmiles(cleaned_mol, kekuleSmiles=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar SMILES '{smile}': {e}\")\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# Funções de Curadoria\n",
    "def remove_salts_stage(df, smiles_col):\n",
    "    \"\"\"Remove sais e moléculas inválidas, atualizando a coluna de SMILES final.\"\"\"\n",
    "    print(\"Removendo sais das moléculas...\")\n",
    "    \n",
    "    # Aplicar a função de remoção de sais e armazenar em uma coluna temporária\n",
    "    df['final_smiles_cleaned'] = df[smiles_col].apply(remove_salts)\n",
    "    \n",
    "    # Identificar os compostos removidos (onde a remoção de sais resultou em None)\n",
    "    removed_salts_df = df[df['final_smiles_cleaned'].isna()].copy()\n",
    "    removed = removed_salts_df.shape[0]\n",
    "    \n",
    "    # Atualizar a coluna 'final_smiles' com os SMILES limpos\n",
    "    df['final_smiles'] = df['final_smiles_cleaned']\n",
    "    \n",
    "    # Remover a coluna temporária\n",
    "    df = df.drop(columns=['final_smiles_cleaned'])\n",
    "    \n",
    "    # Remover os compostos que não possuem SMILES válidos após a remoção de sais\n",
    "    df = df.dropna(subset=['final_smiles']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Removidos {removed} compostos durante a remoção de sais.\")\n",
    "    return df, removed, removed_salts_df\n",
    "\n",
    "def remove_mixtures_stage(df):\n",
    "    \"\"\"Remove misturas (moléculas com pontos), retornando os compostos removidos.\"\"\"\n",
    "    print(\"Removendo misturas de moléculas...\")\n",
    "    initial_count = df.shape[0]\n",
    "    removed_mixtures = df[df['final_smiles'].str.contains(r'\\.')]\n",
    "    removed = removed_mixtures.shape[0]\n",
    "    df = df[~df['final_smiles'].str.contains(r'\\.')].reset_index(drop=True)\n",
    "    print(f\"Removidos {removed} compostos que são misturas.\")\n",
    "    return df, removed, removed_mixtures\n",
    "\n",
    "def standardize_smiles_stage(df, smiles_col):\n",
    "    \"\"\"Padroniza os SMILES.\"\"\"\n",
    "    print(\"Padronizando os SMILES...\")\n",
    "    def standardize_smiles(s):\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(s, sanitize=True)\n",
    "            if mol is None:\n",
    "                return None\n",
    "            mol_block = Chem.MolToMolBlock(mol)\n",
    "            std_mol_block = standardizer.standardize_molblock(mol_block)\n",
    "            std_mol = Chem.MolFromMolBlock(std_mol_block)\n",
    "            return Chem.MolToSmiles(std_mol, kekuleSmiles=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao padronizar SMILES '{s}': {e}\")\n",
    "            return None\n",
    "\n",
    "    df['final_smiles'] = df[smiles_col].apply(standardize_smiles)\n",
    "    removed = df['final_smiles'].isna().sum()\n",
    "    df = df.dropna(subset=['final_smiles']).reset_index(drop=True)\n",
    "    print(f\"Removidos {removed} compostos durante a padronização dos SMILES.\")\n",
    "    return df, removed\n",
    "\n",
    "def calculate_inchi_stage(df):\n",
    "    \"\"\"Calcula InChI para identificação de duplicatas.\"\"\"\n",
    "    print(\"Calculando InChI para as moléculas...\")\n",
    "    def mol_to_inchi(s):\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(s)\n",
    "            if mol:\n",
    "                return Chem.inchi.MolToInchi(mol)\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao calcular InChI para SMILES '{s}': {e}\")\n",
    "            return None\n",
    "\n",
    "    df['InChI'] = df['final_smiles'].apply(mol_to_inchi)\n",
    "    removed = df['InChI'].isna().sum()\n",
    "    df = df.dropna(subset=['InChI']).reset_index(drop=True)\n",
    "    print(f\"Removidos {removed} compostos devido a falhas no cálculo do InChI.\")\n",
    "    return df, removed\n",
    "\n",
    "def remove_duplicates_classification(df, outcome_col):\n",
    "    \"\"\"\n",
    "    Remove duplicatas em datasets de classificação:\n",
    "    - Duplicatas Concordantes: Mantém um exemplar, remove os demais.\n",
    "    - Duplicatas Discordantes: Remove todas as duplicatas.\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame filtrado\n",
    "    - Número de duplicatas concordantes removidas\n",
    "    - Número de duplicatas discordantes removidas\n",
    "    - DataFrame com as duplicatas concordantes removidas\n",
    "    - DataFrame com as duplicatas discordantes removidas\n",
    "    \"\"\"\n",
    "    print(\"Removendo duplicatas para tarefas de classificação...\")\n",
    "    grouped = df.groupby('InChI')\n",
    "    concordant_removed = 0\n",
    "    discordant_removed = 0\n",
    "    keep_indices = []\n",
    "    removed_concordant_rows = []  # Lista para armazenar os concordantes removidos\n",
    "    removed_discordant_rows = []  # Lista para armazenar os discordantes removidos\n",
    "\n",
    "    for inchi, group in grouped:\n",
    "        if len(group) > 1:\n",
    "            # Verificar se todos os Outcomes são iguais\n",
    "            outcomes = group[outcome_col].unique()\n",
    "            if len(outcomes) == 1:\n",
    "                # Duplicatas concordantes: manter apenas um\n",
    "                concordant_removed += (len(group) - 1)\n",
    "                keep_indices.append(group.index[0])\n",
    "                removed = group.iloc[1:]\n",
    "                removed_concordant_rows.append(removed)\n",
    "            else:\n",
    "                # Duplicatas discordantes: remover todas\n",
    "                discordant_removed += len(group)\n",
    "                removed_discordant_rows.append(group)\n",
    "        else:\n",
    "            keep_indices.append(group.index[0])\n",
    "\n",
    "    final_drop_dup = df.loc[keep_indices].reset_index(drop=True)\n",
    "\n",
    "    # Concatenar removidos\n",
    "    if removed_concordant_rows:\n",
    "        removed_concordant_df = pd.concat(removed_concordant_rows, ignore_index=True)\n",
    "    else:\n",
    "        removed_concordant_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    if removed_discordant_rows:\n",
    "        removed_discordant_df = pd.concat(removed_discordant_rows, ignore_index=True)\n",
    "    else:\n",
    "        removed_discordant_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    print(f\"Duplicatas concordantes removidas: {concordant_removed}\")\n",
    "    print(f\"Duplicatas discordantes removidas: {discordant_removed}\")\n",
    "    return final_drop_dup, concordant_removed, discordant_removed, removed_concordant_df, removed_discordant_df\n",
    "\n",
    "def remove_duplicates_regression(df, target_col, threshold=VARIANCE_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Remove duplicatas em datasets de regressão baseado na variação logarítmica:\n",
    "    - Duplicatas Concordantes: Mantém um exemplar com Outcome igual à média dos valores originais.\n",
    "    - Duplicatas Discordantes: Remove todas as duplicatas.\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame filtrado com médias para duplicatas concordantes\n",
    "    - Número de duplicatas concordantes removidas\n",
    "    - Número de duplicatas discordantes removidas\n",
    "    - DataFrame com as duplicatas concordantes removidas\n",
    "    - DataFrame com as duplicatas discordantes removidas\n",
    "    \"\"\"\n",
    "    print(\"Removendo duplicatas para tarefas de regressão...\")\n",
    "    \n",
    "    # Verificar se a coluna contém apenas valores numéricos\n",
    "    df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n",
    "    \n",
    "    # Remover valores não numéricos\n",
    "    invalid_values = df[target_col].isna().sum()\n",
    "    if invalid_values > 0:\n",
    "        print(f\"Atenção: {invalid_values} valores não numéricos encontrados em '{target_col}' e serão removidos.\")\n",
    "        df = df.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    \n",
    "    # Remover valores não positivos (necessário para cálculo de log)\n",
    "    non_positive_values = (df[target_col] <= 0).sum()\n",
    "    if non_positive_values > 0:\n",
    "        print(f\"Atenção: {non_positive_values} valores não positivos encontrados em '{target_col}' e serão removidos.\")\n",
    "        df = df[df[target_col] > 0].reset_index(drop=True)\n",
    "    \n",
    "    # Calcular o logaritmo do target\n",
    "    df['log_target'] = np.log(df[target_col])\n",
    "    \n",
    "    # Agrupar pelo InChI e processar duplicatas\n",
    "    grouped = df.groupby('InChI')\n",
    "    concordant_removed = 0\n",
    "    discordant_removed = 0\n",
    "    keep_rows = []\n",
    "    removed_concordant_rows = []  # Lista para armazenar os concordantes removidos\n",
    "    removed_discordant_rows = []  # Lista para armazenar os discordantes removidos\n",
    "\n",
    "    for inchi, group in grouped:\n",
    "        if len(group) > 1:\n",
    "            std = group['log_target'].std()\n",
    "            if std <= threshold:\n",
    "                # Duplicatas concordantes: calcular média dos Outcomes\n",
    "                mean_outcome = group[target_col].mean()\n",
    "                concordant_removed += (len(group) - 1)\n",
    "                keep = group.iloc[0].copy()\n",
    "                keep[target_col] = mean_outcome\n",
    "                keep_rows.append(keep)\n",
    "                # Remover as demais entradas\n",
    "                removed = group.iloc[1:]\n",
    "                removed_concordant_rows.append(removed)\n",
    "            else:\n",
    "                # Duplicatas discordantes: remover todas\n",
    "                discordant_removed += len(group)\n",
    "                removed_discordant_rows.append(group)\n",
    "        else:\n",
    "            keep_rows.append(group.iloc[0])\n",
    "\n",
    "    # Criar DataFrame filtrado\n",
    "    if keep_rows:\n",
    "        filtered_df = pd.DataFrame(keep_rows).drop(columns=['log_target']).reset_index(drop=True)\n",
    "    else:\n",
    "        filtered_df = pd.DataFrame(columns=df.columns.drop('log_target'))\n",
    "\n",
    "    # Concatenar removidos\n",
    "    if removed_concordant_rows:\n",
    "        removed_concordant_df = pd.concat(removed_concordant_rows, ignore_index=True)\n",
    "    else:\n",
    "        removed_concordant_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    if removed_discordant_rows:\n",
    "        removed_discordant_df = pd.concat(removed_discordant_rows, ignore_index=True)\n",
    "    else:\n",
    "        removed_discordant_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "    print(f\"Duplicatas concordantes removidas: {concordant_removed}\")\n",
    "    print(f\"Duplicatas discordantes removidas: {discordant_removed}\")\n",
    "    return filtered_df, concordant_removed, discordant_removed, removed_concordant_df, removed_discordant_df\n",
    "\n",
    "def write_log(savepath, log_data):\n",
    "    \"\"\"Escreve o log do processo de curadoria.\"\"\"\n",
    "    try:\n",
    "        log_path = os.path.join(savepath, 'curation_log.txt')\n",
    "        print(f\"Salvando log em '{log_path}'...\")\n",
    "        with open(log_path, 'w') as log_file:\n",
    "            log_file.write('Log do processo de curadoria:\\n')\n",
    "            for key, value in log_data.items():\n",
    "                log_file.write(f'{key}: {value}\\n')\n",
    "        print(\"Log salvo com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar o log: {e}\")\n",
    "\n",
    "def curate_dataset(df, smiles_col, outcome_col, task_type='classification', savepath='curated_data'):\n",
    "    \"\"\"\n",
    "    Função principal de curadoria que executa as etapas fixas de curadoria:\n",
    "    - Remove NaNs de Outcome e SMILES\n",
    "    - Prepara os SMILES removendo estereoquímica e valida\n",
    "    - Padroniza estruturalmente os SMILES\n",
    "    - Remove sais da molécula\n",
    "    - Remove misturas da molécula\n",
    "    - Remove duplicatas conforme o tipo de tarefa (classificação ou regressão)\n",
    "\n",
    "    Parâmetros:\n",
    "    - df (pd.DataFrame): DataFrame contendo os dados.\n",
    "    - smiles_col (str): Nome da coluna que contém os SMILES das moléculas.\n",
    "    - outcome_col (str): Nome da coluna que contém os resultados (Outcome).\n",
    "    - task_type (str): Tipo de tarefa, 'classification' ou 'regression'. Padrão é 'classification'.\n",
    "    - savepath (str): Diretório onde os arquivos curados e logs serão salvos. Padrão é 'curated_data'.\n",
    "\n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame curado.\n",
    "    - dict: Dicionário contendo informações do log.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Inicialização do log\n",
    "        log_data = {}\n",
    "\n",
    "        # Certificar que o diretório de salvamento existe\n",
    "        if not os.path.exists(savepath):\n",
    "            print(f\"Criando diretório de salvamento em '{savepath}'...\")\n",
    "            os.makedirs(savepath, exist_ok=True)\n",
    "            print(\"Diretório criado.\")\n",
    "\n",
    "        # Capturar quantidade inicial de compostos\n",
    "        initial_count = df.shape[0]\n",
    "        log_data['Initial Compounds'] = initial_count\n",
    "        print(f\"Quantidade inicial de compostos: {initial_count}\")\n",
    "\n",
    "        # Remover valores nulos em Outcome e SMILES\n",
    "        print(\"Removendo valores nulos em Outcome e SMILES...\")\n",
    "        df = df.dropna(subset=[smiles_col, outcome_col]).reset_index(drop=True)\n",
    "        removed_after_nan = initial_count - df.shape[0]\n",
    "        log_data['Removed After Drop NaN Molecule/Outcome'] = removed_after_nan\n",
    "        print(f\"Removidos {removed_after_nan} compostos devido a NaNs.\")\n",
    "\n",
    "        # Preparar os SMILES removendo estereoquímica e validando\n",
    "        print(\"Preparando os SMILES removendo estereoquímica e validando...\")\n",
    "        def prepare_smiles(smile):\n",
    "            \"\"\"Remove estereoquímica e valida o SMILES.\"\"\"\n",
    "            try:\n",
    "                cleaned_smile = smile.replace('@', '').replace('/', '').replace('\\\\', '')\n",
    "                mol = Chem.MolFromSmiles(cleaned_smile)\n",
    "                if mol:\n",
    "                    return Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                else:\n",
    "                    return None\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao preparar SMILES '{smile}': {e}\")\n",
    "                return None\n",
    "\n",
    "        df['final_smiles'] = df[smiles_col].apply(prepare_smiles)\n",
    "        removed_invalid_smiles = df['final_smiles'].isna().sum()\n",
    "        df = df.dropna(subset=['final_smiles']).reset_index(drop=True)\n",
    "        log_data['Invalid SMILES Removed During Preparation'] = removed_invalid_smiles\n",
    "        print(f\"Removidos {removed_invalid_smiles} compostos com SMILES inválidos durante a preparação.\")\n",
    "        print(f\"Quantidade de compostos após preparação dos SMILES: {df.shape[0]}\")\n",
    "\n",
    "        # Padronizar SMILES\n",
    "        df, removed = standardize_smiles_stage(df, 'final_smiles')\n",
    "        log_data['SMILES Standardized Removed'] = removed\n",
    "        print(f\"Quantidade de compostos após padronização dos SMILES: {df.shape[0]}\")\n",
    "\n",
    "        # Remover sais\n",
    "        df, removed_salts, removed_salts_df = remove_salts_stage(df, 'final_smiles')\n",
    "        log_data['Salts Removed'] = removed_salts\n",
    "        print(f\"Quantidade de compostos após remoção de sais: {df.shape[0]}\")\n",
    "\n",
    "        # Salvar sais removidos\n",
    "        if not removed_salts_df.empty:\n",
    "            removed_salts_csv = os.path.join(savepath, 'removed_salts.csv')\n",
    "            print(f\"Salvando sais removidos em '{removed_salts_csv}'...\")\n",
    "            removed_salts_df.to_csv(removed_salts_csv, sep=',', header=True, index=False)\n",
    "            print(\"Sais removidos salvos com sucesso.\")\n",
    "        else:\n",
    "            print(\"Nenhum sal removido para salvar.\")\n",
    "\n",
    "        # Remover misturas\n",
    "        df, removed_mixtures, removed_mixtures_df = remove_mixtures_stage(df)\n",
    "        log_data['Mixtures Removed'] = removed_mixtures\n",
    "        print(f\"Quantidade de compostos após remoção de misturas: {df.shape[0]}\")\n",
    "\n",
    "        # Salvar misturas removidas\n",
    "        if not removed_mixtures_df.empty:\n",
    "            removed_mixtures_csv = os.path.join(savepath, 'removed_mixtures.csv')\n",
    "            print(f\"Salvando misturas removidas em '{removed_mixtures_csv}'...\")\n",
    "            removed_mixtures_df.to_csv(removed_mixtures_csv, sep=',', header=True, index=False)\n",
    "            print(\"Misturas removidas salvas com sucesso.\")\n",
    "        else:\n",
    "            print(\"Nenhuma mistura removida para salvar.\")\n",
    "\n",
    "        # Calcular InChI\n",
    "        df, removed = calculate_inchi_stage(df)\n",
    "        log_data['InChI Calculation Removed'] = removed\n",
    "        print(f\"Quantidade de compostos após cálculo de InChI: {df.shape[0]}\")\n",
    "\n",
    "        # Remover duplicatas conforme o tipo de tarefa\n",
    "        if task_type == 'classification':\n",
    "            df, concordant, discordant, removed_concordant_dup, removed_discordant_dup = remove_duplicates_classification(df, outcome_col)\n",
    "            log_data['Concordant Duplicates Removed'] = concordant\n",
    "            log_data['Discordant Duplicates Removed'] = discordant\n",
    "            print(f\"Quantidade de compostos após remoção de duplicatas (classificação): {df.shape[0]}\")\n",
    "        elif task_type == 'regression':\n",
    "            df, concordant, discordant, removed_concordant_dup, removed_discordant_dup = remove_duplicates_regression(df, outcome_col)\n",
    "            log_data['Concordant Duplicates Removed'] = concordant\n",
    "            log_data['Discordant Duplicates Removed'] = discordant\n",
    "            print(f\"Quantidade de compostos após remoção de duplicatas (regressão): {df.shape[0]}\")\n",
    "        else:\n",
    "            raise ValueError(\"task_type deve ser 'classification' ou 'regression'.\")\n",
    "\n",
    "        # Capturar quantidade final de compostos\n",
    "        final_count = df.shape[0]\n",
    "        log_data['Final Compounds'] = final_count\n",
    "        print(f\"Quantidade final de compostos: {final_count}\")\n",
    "\n",
    "        # Salvar dataset final\n",
    "        output_csv = os.path.join(savepath, 'curated_dataset.csv')\n",
    "        print(f\"Salvando dataset curado em '{output_csv}'...\")\n",
    "        df.to_csv(output_csv, sep=',', header=True, index=False)\n",
    "        print(\"Dataset curado salvo com sucesso.\")\n",
    "\n",
    "        # Salvar os duplicatas removidos separadamente\n",
    "        if task_type == 'classification':\n",
    "            # Salvar duplicatas concordantes removidas\n",
    "            if not removed_concordant_dup.empty:\n",
    "                removed_concordant_csv = os.path.join(savepath, 'removed_concordant_duplicates.csv')\n",
    "                print(f\"Salvando duplicatas concordantes removidas em '{removed_concordant_csv}'...\")\n",
    "                removed_concordant_dup.to_csv(removed_concordant_csv, sep=',', header=True, index=False)\n",
    "                print(\"Duplicatas concordantes removidas salvas com sucesso.\")\n",
    "            else:\n",
    "                print(\"Nenhuma duplicata concordante removida para salvar.\")\n",
    "\n",
    "            # Salvar duplicatas discordantes removidas\n",
    "            if not removed_discordant_dup.empty:\n",
    "                removed_discordant_csv = os.path.join(savepath, 'removed_discordant_duplicates.csv')\n",
    "                print(f\"Salvando duplicatas discordantes removidas em '{removed_discordant_csv}'...\")\n",
    "                removed_discordant_dup.to_csv(removed_discordant_csv, sep=',', header=True, index=False)\n",
    "                print(\"Duplicatas discordantes removidas salvas com sucesso.\")\n",
    "            else:\n",
    "                print(\"Nenhuma duplicata discordante removida para salvar.\")\n",
    "\n",
    "        elif task_type == 'regression':\n",
    "            # Salvar duplicatas concordantes removidas\n",
    "            if not removed_concordant_dup.empty:\n",
    "                removed_concordant_csv = os.path.join(savepath, 'removed_concordant_duplicates.csv')\n",
    "                print(f\"Salvando duplicatas concordantes removidas em '{removed_concordant_csv}'...\")\n",
    "                removed_concordant_dup.to_csv(removed_concordant_csv, sep=',', header=True, index=False)\n",
    "                print(\"Duplicatas concordantes removidas salvas com sucesso.\")\n",
    "            else:\n",
    "                print(\"Nenhuma duplicata concordante removida para salvar.\")\n",
    "\n",
    "            # Salvar duplicatas discordantes removidas\n",
    "            if not removed_discordant_dup.empty:\n",
    "                removed_discordant_csv = os.path.join(savepath, 'removed_discordant_duplicates.csv')\n",
    "                print(f\"Salvando duplicatas discordantes removidas em '{removed_discordant_csv}'...\")\n",
    "                removed_discordant_dup.to_csv(removed_discordant_csv, sep=',', header=True, index=False)\n",
    "                print(\"Duplicatas discordantes removidas salvas com sucesso.\")\n",
    "            else:\n",
    "                print(\"Nenhuma duplicata discordante removida para salvar.\")\n",
    "\n",
    "        # Escrever o log\n",
    "        write_log(savepath, log_data)\n",
    "\n",
    "        # Retornar dataframe e log\n",
    "        return df, log_data\n",
    "    except Exception as e:\n",
    "        print(f'Erro durante a curadoria do dataset: {e}')\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o arquivo Excel ou CSV\n",
    "df = pd.read_excel('AID_1345082_datatable.csv')\n",
    "# ou se for CSV:\n",
    "#df = pd.read_csv('/EC3LLNA ready.csv')\n",
    "\n",
    "df  # Para verificar o início do dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_df, log = curate_dataset(\n",
    "    df, \n",
    "    smiles_col='PUBCHEM_EXT_DATASOURCE_SMILES',        # Substitua pelo nome da sua coluna de SMILES\n",
    "    outcome_col='Potency',               # Substitua pelo nome da sua coluna de Outcome\n",
    "    task_type='regression',               # Ou 'classification' conforme sua necessidade\n",
    "    savepath='/Users/franciscolucas/Downloads/'  # Diretório correto\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-rdkit-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
