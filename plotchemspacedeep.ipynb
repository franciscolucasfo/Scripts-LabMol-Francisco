{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import numpy as np\n",
    "import umap\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the set file\n",
    "set_file_path = r'C:\\Users\\franc\\Desktop\\teste\\*.csv'\n",
    "csv_files = glob.glob(set_file_path)\n",
    "datasets = []\n",
    "# Iterate through each CSV file and read it as a dataframe\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file)\n",
    "    datasets.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m indice, conjunto_dados \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(datasets):\n\u001b[0;32m     51\u001b[0m         figura\u001b[38;5;241m.\u001b[39madd_trace(go\u001b[38;5;241m.\u001b[39mScatter(x\u001b[38;5;241m=\u001b[39mumap_cls[indice, \u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mumap_cls[indice, \u001b[38;5;241m1\u001b[39m], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarkers\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39mconjunto_dados\u001b[38;5;241m.\u001b[39mfile_name))\n\u001b[1;32m---> 53\u001b[0m \u001b[43mplot_umap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 33\u001b[0m, in \u001b[0;36mplot_umap\u001b[1;34m(datasets)\u001b[0m\n\u001b[0;32m     30\u001b[0m lote_mascara_atencao \u001b[38;5;241m=\u001b[39m mascara_atencao[inicio:fim]\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 33\u001b[0m     saidas \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlote_ids_entrada\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlote_mascara_atencao\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Obter embeddings para todos os tokens, não apenas o token CLS\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m saidas\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
      "File \u001b[1;32mc:\\Users\\franc\\anaconda3\\envs\\ia\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\franc\\anaconda3\\envs\\ia\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\franc\\anaconda3\\envs\\ia\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1103\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1096\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m _prepare_4d_causal_attention_mask_for_sdpa(\n\u001b[0;32m   1097\u001b[0m             attention_mask,\n\u001b[0;32m   1098\u001b[0m             input_shape,\n\u001b[0;32m   1099\u001b[0m             embedding_output,\n\u001b[0;32m   1100\u001b[0m             past_key_values_length,\n\u001b[0;32m   1101\u001b[0m         )\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1103\u001b[0m         extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_4d_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_length\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[0;32m   1109\u001b[0m     extended_attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extended_attention_mask(attention_mask, input_shape)\n",
      "File \u001b[1;32mc:\\Users\\franc\\anaconda3\\envs\\ia\\lib\\site-packages\\transformers\\modeling_attn_mask_utils.py:451\u001b[0m, in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[1;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;66;03m# torch.jit.trace, symbolic_trace and torchdynamo with fullgraph=True are unable to capture the controlflow `is_causal=attention_mask is None and q_len > 1`\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# used as an SDPA argument. We keep compatibility with these tracing tools by always using SDPA's `attn_mask` argument in case we are tracing.\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# TODO: For dynamo, rather use a check on fullgraph=True once this is possible (https://github.com/pytorch/pytorch/pull/120400).\u001b[39;00m\n\u001b[0;32m    445\u001b[0m is_tracing \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    446\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing()\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mProxy)\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(torch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_dynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mis_compiling())\n\u001b[0;32m    449\u001b[0m )\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tgt_len \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;66;03m# For query_length == 1, causal attention and bi-directional attention are the same.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def plot_umap(datasets):\n",
    "\n",
    "    # Definir parâmetros UMAP\n",
    "    numero_vizinhos = 10\n",
    "    distancia_minima = 0.1\n",
    "    metrica = 'euclidean'\n",
    "\n",
    "    # Definir cores para cada conjunto de dados\n",
    "    cores = ['red', 'green', 'blue', 'purple', 'orange', 'brown', 'pink', 'cyan', 'gray', 'olive', 'yellow', 'black', 'teal', 'indigo', 'violet', 'wheat', 'navy', 'silver', 'maroon', 'lime', 'coral', 'darkblue', 'darkred']\n",
    "\n",
    "    # Inicializar listas vazias para armazenar embeddings\n",
    "    embeddings_cls = []\n",
    "    embeddings_media_ponderada = []\n",
    "\n",
    "    # Iterar através de cada conjunto de dados\n",
    "    for indice, conjunto_dados in enumerate(datasets):\n",
    "        # Tokenizar sequências de entrada e preencher sequências\n",
    "        ids_entrada = [tokenizer.encode(texto, add_special_tokens=True, padding='max_length', max_length=128, truncation=True) for texto in conjunto_dados['final_smiles']]\n",
    "        ids_entrada = torch.tensor(ids_entrada).to(device)\n",
    "\n",
    "        # Criar máscaras de atenção\n",
    "        mascara_atencao = [[float(i != 0.0) for i in seq] for seq in ids_entrada]\n",
    "        mascara_atencao = torch.tensor(mascara_atencao).to(device)\n",
    "\n",
    "        # Reduzir o tamanho do lote se necessário\n",
    "        tamanho_lote = 20  # ou qualquer número que seja adequado para sua GPU\n",
    "        for inicio in range(0, len(ids_entrada), tamanho_lote):\n",
    "            fim = inicio + tamanho_lote\n",
    "            lote_ids_entrada = ids_entrada[inicio:fim]\n",
    "            lote_mascara_atencao = mascara_atencao[inicio:fim]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                saidas = model(lote_ids_entrada, attention_mask=lote_mascara_atencao)\n",
    "            \n",
    "                # Obter embeddings para todos os tokens, não apenas o token CLS\n",
    "                embeddings = saidas.last_hidden_state\n",
    "            \n",
    "                # Pooling médio\n",
    "                somatorio_embeddings = torch.sum(embeddings * lote_mascara_atencao.unsqueeze(-1), 1)\n",
    "                embeddings_media_ponderada.append(somatorio_embeddings / lote_mascara_atencao.sum(1).unsqueeze(-1))\n",
    "\n",
    "    # Aplicar UMAP\n",
    "    umap_cls = umap.UMAP(n_neighbors=numero_vizinhos, min_dist=distancia_minima, metric=metrica).fit_transform(embeddings_cls)\n",
    "    umap_media_ponderada = umap.UMAP(n_neighbors=numero_vizinhos, min_dist=distancia_minima, metric=metrica).fit_transform(embeddings_media_ponderada)\n",
    "\n",
    "    # Plotar visualização UMAP\n",
    "    figura = go.Figure()\n",
    "\n",
    "    # Adicionar rastros à figura para cada conjunto de dados\n",
    "    for indice, conjunto_dados in enumerate(datasets):\n",
    "        figura.add_trace(go.Scatter(x=umap_cls[indice, 0], y=umap_cls[indice, 1], mode='markers', name=conjunto_dados.file_name))\n",
    "\n",
    "plot_umap(datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
